from algo import Algorithm

from lagom.experiment import Config
from lagom.experiment import BaseExperimentWorker
from lagom.experiment import BaseExperimentMaster


class ExperimentWorker(BaseExperimentWorker):
    def make_algo(self):
        algo = Algorithm(name='Policy gradient in CartPole')
        
        return algo


class ExperimentMaster(BaseExperimentMaster):
    def process_algo_result(self, config, result):
        assert result is None
        
    def make_configs(self):
        config = Config()
        
        config.add_grid(name='cuda', val=[True])
        # Random seeds
        # Generated by: 
        # import numpy as np
        # np.random.randint(0, np.iinfo(np.int32).max, 10)
        config.add_grid(name='seed', val=[1284204222, 1079618558, 310837894, 
                                          1130644153, 2099771862, 1234806135,
                                          92464293, 146053987, 1140885110,
                                          988661500])
        
        config.add_item(name='lr', val=3e-2)
        config.add_item(name='gamma', val=0.99)
        config.add_item(name='standardize_pg', val=False)
        config.add_item(name='train_iter', val=700)
        config.add_item(name='N', val=1)
        config.add_item(name='T', val=200)
        
        config.add_item(name='use_value', val=True)  # True for actor-critic
        config.add_item(name='value_coef', val=0.5)  # Coefficient for learning value function
        config.add_item(name='entropy_coef', val=0.01)  # Coefficient for maximize policy entropy
        
        config.add_item(name='max_grad_norm', val=0.5)  # gradient clipping with max gradient norm
        
        config.add_item(name='log_interval', val=100)
        
        configs = config.make_configs()
        
        return configs
