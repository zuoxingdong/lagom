{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution strategies on OpenAI gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create policy network for Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lagom.agents import BaseAgent\n",
    "from lagom.envs import EnvSpec, GymEnv\n",
    "from lagom.core.networks import BaseMLP\n",
    "from lagom.core.policies import BaseGaussianPolicy\n",
    "\n",
    "\n",
    "class MLP(BaseMLP):\n",
    "    def make_params(self, config):\n",
    "        self.fc1 = nn.Linear(in_features=3, out_features=32)\n",
    "\n",
    "        self.mean_head = nn.Linear(in_features=32, out_features=1)\n",
    "        self.logvar_head = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "    def init_params(self, config):\n",
    "        gain = nn.init.calculate_gain(nonlinearity='relu')\n",
    "\n",
    "        nn.init.orthogonal_(self.fc1.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "\n",
    "        nn.init.orthogonal_(self.mean_head.weight, gain=gain)\n",
    "        nn.init.constant_(self.mean_head.bias, 0.0)\n",
    "\n",
    "        nn.init.orthogonal_(self.logvar_head.weight, gain=gain)\n",
    "        nn.init.constant_(self.logvar_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        mean = self.mean_head(x)\n",
    "        logvar = self.logvar_head(x)\n",
    "\n",
    "        # Output dictionary\n",
    "        out = {}\n",
    "        out['mean'] = mean\n",
    "        out['logvar'] = logvar\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class GaussianPolicy(BaseGaussianPolicy):\n",
    "    def process_network_output(self, network_out):\n",
    "        return {}\n",
    "\n",
    "    def constraint_action(self, action):\n",
    "        return 2*torch.tanh(action)\n",
    "    \n",
    "    \n",
    "class Agent(BaseAgent):\n",
    "    def __init__(self, policy, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.policy = policy\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        obs = obs.unsqueeze(0)\n",
    "        \n",
    "        output_policy = self.policy(obs)\n",
    "        \n",
    "        action = output_policy['action']\n",
    "        action = 2*torch.tanh(action)\n",
    "        action = action.squeeze(0)\n",
    "        \n",
    "        output = {}\n",
    "        output['action'] = action\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def make_env(seed=None, monitor=False, monitor_dir=None):\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    if monitor:\n",
    "        env = Monitor(env, directory=monitor_dir)\n",
    "    env = GymEnv(env)\n",
    "    \n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP().num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lagom.envs import EnvSpec\n",
    "from lagom.runner import Runner\n",
    "\n",
    "def rollout(parameters, env, N, T):\n",
    "    parameters = torch.from_numpy(parameters).float()\n",
    "    env_spec = EnvSpec(env)\n",
    "    \n",
    "    # Create a network\n",
    "    network = MLP(config=None)\n",
    "    # Load parameters to the network\n",
    "    network.from_vec(parameters)\n",
    "    # Create a Gaussian policy\n",
    "    policy = GaussianPolicy(network=network, env_spec=env_spec)\n",
    "    # Create an Agent\n",
    "    agent = Agent(policy=policy, config=None)\n",
    "    \n",
    "    # Create runner\n",
    "    runner = Runner(agent=agent, env=env, gamma=1.0)\n",
    "    # Make rollouts\n",
    "    D = runner(N=N, T=T)\n",
    "    \n",
    "    mean_return = np.mean([trajectory.all_returns[0] for trajectory in D])\n",
    "    \n",
    "    # Negate the reward, because ES is doing minimization. \n",
    "    function_value = -mean_return\n",
    "    \n",
    "    return function_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create master-worker classes for ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30_w,60)-aCMA-ES (mu_w=16.6,w_1=12%) in dimension 194 (seed=492886, Thu Jul 12 12:54:13 2018)\n",
      "Best function value at generation 1: -274.81550405025484\n",
      "Best function value at generation 100: -30.490639524441214\n",
      "Best function value at generation 200: -26.765747442189603\n",
      "Best function value at generation 300: -24.436559721687807\n",
      "Best function value at generation 400: -3.8921948190778495\n",
      "Best function value at generation 500: -3.8921948190778495\n",
      "Best function value at generation 600: -3.8921948190778495\n",
      "Best function value at generation 700: -1.0937648743391037\n",
      "Best function value at generation 800: -1.0937648743391037\n",
      "Best function value at generation 900: -1.0937648743391037\n",
      "Best function value at generation 1000: -1.0937648743391037\n",
      "Total time: 3.24e+02 s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from lagom.core.es import CMAES, OpenAIES\n",
    "\n",
    "from lagom.core.es import BaseESWorker\n",
    "from lagom.core.es import BaseGymESMaster\n",
    "\n",
    "\n",
    "class ESWorker(BaseESWorker):\n",
    "    def f(self, solution, seed):\n",
    "        solution, make_env = solution\n",
    "        \n",
    "        # Create an environment and seed it\n",
    "        env = make_env(seed)\n",
    "        \n",
    "        # Evaluate the solution\n",
    "        function_value = rollout(parameters=solution, \n",
    "                                 env=env, \n",
    "                                 N=5, \n",
    "                                 T=50)\n",
    "        \n",
    "        return function_value\n",
    "    \n",
    "\n",
    "class ESMaster(BaseGymESMaster):\n",
    "    def make_es(self):\n",
    "        cmaes = CMAES(mu0=[0]*194, \n",
    "                      std0=0.5, \n",
    "                      popsize=60)\n",
    "        \n",
    "        return cmaes\n",
    "        \n",
    "    def _process_es_result(self, result):\n",
    "        best_f_val = result['best_f_val']\n",
    "        if self.generation == 0 or (self.generation+1) % 100 == 0:\n",
    "            best_f_val = -best_f_val  # negate to get back reward\n",
    "            print(f'Best function value at generation {self.generation+1}: {best_f_val}')\n",
    "            \n",
    "        # Save the parameters in final generation\n",
    "        if (self.generation+1) == self.num_iteration:\n",
    "            np.save('trained_param', result['best_param'])\n",
    "            \n",
    "            \n",
    "t = time()\n",
    "\n",
    "es = ESMaster(make_env=make_env,\n",
    "              num_iteration=1000, \n",
    "              worker_class=ESWorker, \n",
    "              num_worker=60, \n",
    "              init_seed=0, \n",
    "              daemonic_worker=None)\n",
    "es()\n",
    "\n",
    "print(f'Total time: {time() - t:.3} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.986847748979926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved parameter\n",
    "parameters = np.load('trained_param.npy')\n",
    "\n",
    "# Make environment\n",
    "env = make_env(seed=None, monitor=True, monitor_dir='logs/')\n",
    "        \n",
    "# Evaluate the solution\n",
    "function_value = rollout(parameters=parameters, \n",
    "                         env=env, \n",
    "                         N=1, \n",
    "                         T=50)\n",
    "function_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
