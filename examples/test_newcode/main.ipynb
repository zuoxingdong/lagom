{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP: Import lagom\n",
    "# Not useful once lagom is installed\n",
    "import sys\n",
    "sys.path.append('/home/zuo/Code/lagom/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use functiontools.partial to set make_env function without argument but internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from '/home/zuo/anaconda3/envs/RL_server/lib/python3.6/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.swapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Process  # easier code than threading\n",
    "from multiprocessing import Pipe  # Much faster than Queue\n",
    "\n",
    "\n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.x()\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "\n",
    "def worker(child_conn, parent_conn, make_env):\n",
    "    parent_conn.close()\n",
    "    # Create an environment\n",
    "    env = make_env()\n",
    "    \n",
    "    while True:\n",
    "        cmd, data = child_conn.recv()\n",
    "        if cmd == 'step':\n",
    "            obs, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                obs = env.reset()  # TODO: why reset\n",
    "            child_conn.send([obs, reward, done, info])\n",
    "        elif cmd == 'reset':\n",
    "            obs = env.reset()\n",
    "            child_conn.send(obs)\n",
    "        elif cmd == 'reset_task':\n",
    "            obs = env.reset_task()\n",
    "            child_conn.send(obs)\n",
    "        elif cmd == 'close':\n",
    "            child_conn.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            child_conn.send([env.observation_space, env.action_space])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class SubprocVecEnv(VecEnv):\n",
    "    \"\"\"\n",
    "    Run a list of environment in subprocesses\n",
    "    \"\"\"\n",
    "    def __init__(self, list_make_env):\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        self.num_envs = len(list_make_env)\n",
    "        self.parent_conns, self.child_conns = zip(*[Pipe() for _ in range(self.num_envs)])\n",
    "        self.processes = []\n",
    "        for parent_conn, child_conn, make_env in zip(self.parent_conns, self.child_conns, list_make_env):\n",
    "            self.processes.append(Process(target=worker, args=[child_conn, parent_conn, CloudpickleWrapper(make_env)]))  # TODO: CloudpickleWrapper make_env\n",
    "        for process in self.processes:\n",
    "            process.daemon = True  # if the main process crashes, we should not cause things to hang\n",
    "            process.start()\n",
    "        for conn in self.child_conns:\n",
    "            conn.close()\n",
    "        \n",
    "        # Obtain observation and action spaces\n",
    "        self.parent_conns[0].send(['get_spaces', None])\n",
    "        observation_space, action_space = self.parent_conns[0].recv()\n",
    "        super().__init__(self.num_envs, observation_space, action_space)\n",
    "        \n",
    "    def step_asyn(self, actions):\n",
    "        for parent_conn, action in zip(self.parent_conns, actions):\n",
    "            parent_conn.send(['step', action])\n",
    "            \n",
    "        self.waiting = True\n",
    "        \n",
    "    def step_wait(self):\n",
    "        observations, rewards, dones, infos = zip(*[parent_conn.recv() for parent_conn in self.parent_conns])\n",
    "        self.waiting = False\n",
    "        return np.stack(observations), np.stack(rewards), np.stack(dones), infos\n",
    "    \n",
    "    def reset(self):\n",
    "        for parent_conn in self.parent_conns:\n",
    "            parent_conn.send(['reset', None])\n",
    "        return np.stack([parent_conn.recv() for parent_conn in self.parent_conns])\n",
    "    \n",
    "    def reset_task(self):\n",
    "        for parent_conn in self.parent_conns:\n",
    "            parent_conn.send(['reset_task', None])\n",
    "        return np.stack([parent_conn.recv() for parent_conn in self.parent_conns])\n",
    "    \n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for parent_conn in self.parent_conns:\n",
    "                parent_conn.recv()\n",
    "        for parent_conn in self.parent_conns:\n",
    "            parent_conn.send(['close', None])\n",
    "        for process in self.processes:\n",
    "            process.join()\n",
    "        self.closed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(x/T).astype('float32')\n",
    "\n",
    "\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "        \n",
    "    def forward(self, x, future=0):\n",
    "        outputs = []\n",
    "        batch_size, x_size = x.size()\n",
    "        \n",
    "        h_t = torch.zeros(batch_size, 51)\n",
    "        c_t = torch.zeros(batch_size, 51)\n",
    "        h_t_2 = torch.zeros(batch_size, 51)\n",
    "        c_t_2 = torch.zeros(batch_size, 51)\n",
    "        \n",
    "        chunks = x.chunk(x_size, dim=1)\n",
    "        \n",
    "        for x_t in chunks:\n",
    "            h_t, c_t = self.lstm1(x_t, (h_t, c_t))\n",
    "            h_t_2, c_t_2 = self.lstm2(h_t, (h_t_2, c_t_2))\n",
    "            \n",
    "            output = self.linear(h_t_2)\n",
    "            \n",
    "            outputs.append(output)\n",
    "            \n",
    "        for _ in range(future):  # if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t_2, c_t_2 = self.lstm2(h_t, (h_t_2, c_t_2))\n",
    "            \n",
    "            output = self.linear(h_t_2)\n",
    "            \n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = torch.stack(outputs, dim=1).squeeze(2)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    input = torch.from_numpy(data[3:, :-1])\n",
    "    target = torch.from_numpy(data[3:, 1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3, 1:])\n",
    "\n",
    "    seq = Sequence()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Use LBFGS since we load whole data to train\n",
    "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "\n",
    "    for i in range(2):\n",
    "        print(f'STEP: {i}')\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "            print(f'Loss: {loss.item()}')\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # Prediction\n",
    "        future = 1000\n",
    "        pred = seq(test_input, future=future)\n",
    "        loss = criterion(pred[:, :-future], test_target)\n",
    "        print(f'Test loss: {loss.item()}')\n",
    "        y = pred.data.numpy()\n",
    "\n",
    "        # Drawing\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth=2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1)+future), yi[input.size(1):], color + ':', linewidth=2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        \n",
    "        plt.savefig(f'logs/{i}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP: Import lagom\n",
    "# Not useful once lagom is installed\n",
    "import sys\n",
    "sys.path.append('/home/zuo/Code/lagom/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration #1: 546.5628051757812\n",
      "Training iteration #100: 177.91822814941406\n",
      "Training iteration #200: 151.71519470214844\n",
      "Training iteration #300: 138.11561584472656\n",
      "Training iteration #400: 135.24795532226562\n",
      "====> Average loss: 165.32561080729167\n",
      "====> Test set loss: 119.13991490478516\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(config['seed'])\n",
    "\n",
    "# Automatic check if there is GPU\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Define device\n",
    "config['device'] = torch.device('cuda' if cuda else 'cpu')\n",
    "device = config['device']\n",
    "\n",
    "# Define GPU-dependent keywords for DataLoader\n",
    "if cuda:\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "else:\n",
    "    kwargs = {}\n",
    "    \n",
    "# Create dataset for training and testing\n",
    "train_dataset = datasets.MNIST('data/', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('data/', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=config['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          **kwargs)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=config['batch_size'], \n",
    "                         shuffle=True, \n",
    "                         **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "from lagom.core.networks import VAE\n",
    "\n",
    "model = VAE(input_dim=784, \n",
    "            encoder_sizes=[400], \n",
    "            encoder_nonlinearity=F.relu, \n",
    "            latent_dim=20, \n",
    "            decoder_sizes=[400], \n",
    "            decoder_nonlinearity=F.relu)\n",
    "model = model.to(config['device'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "       \n",
    "from lagom.core.utils import Logger\n",
    "logger = Logger(name='logger')\n",
    "engine = Engine(model, optimizer, train_loader, test_loader, config, logger)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    engine.train()\n",
    "    engine.eval()\n",
    "    \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'data/sample_' + str(epoch) + '.png')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['batch_size'] = 128\n",
    "config['epochs'] = 1\n",
    "config['seed'] = 1\n",
    "config['log_interval'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "====> Epoch: 1 Average loss: 164.4686\n",
    "====> Test set loss: 119.2985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "165.3256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
